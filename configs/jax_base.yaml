# JAX configuration for CatGPT training
# Override values via CLI: uv run python scripts/train_jax.py model.hidden_size=512

# Model architecture
model:
  name: "transformer"
  hidden_size: 512
  num_layers: 16
  num_heads: 32
  ff_dim: 1536  # Defaults to 4 * hidden_size
  vocab_size: 28  # From tokenizer (don't change unless tokenizer changes)
  seq_length: 64  # Will be overridden by tokenizer.sequence_length
  activation: "gelu"

  # Smolgen: dynamic attention bias generation
  # Generates position-dependent attention biases conditioned on input
  # See: https://lczero.org/blog/2024/02/transformer-progress/
  smolgen:
    enabled: false
    hidden_channels: 32  # Compression dimension
    hidden_size: 256     # Hidden layer size
    gen_size: 256        # Per-head generation dimension

  # Output heads configuration
  output_heads:
    self_head: true      # Token reconstruction (auxiliary task for stability)
    value_head: true     # Win probability prediction (primary task)
    policy_head: true    # Policy prediction (primary task)
    self_weight: 0.1     # Loss weight for self head
    value_weight: 1.0    # Loss weight for value head
    policy_weight: 1.0    # Loss weight for policy head
    soft_policy_head: false  # Enable soft policy
    soft_policy_temperature: 4.0  # KataGo default
    soft_policy_weight: 8.0  # KataGo default (compensates for smaller gradients)
    # Square prediction heads (noisy auxiliary targets)
    next_capture_head: true  # Predict square of piece to be captured next
    next_capture_weight: 1.0  # Small weight for noisy auxiliary target
    next_pawn_move_head: false  # Predict square of pawn that will move next
    next_pawn_move_weight: 0.1  # Small weight for noisy auxiliary target

# Tokenizer configuration
tokenizer:
  sequence_length: 64
  include_halfmove: false

# Optimizer configuration
optimizer:
  name: "splus"  # Options: "adamw", "splus", "sgd"
  learning_rate: 0.095
  weight_decay: 0.01
  # AdamW/SGD specific
  b1: 0.9
  b2: 0.999
  eps: 1.0e-8
  # SPlus specific (only used when name="splus")
  splus_b1: 0.9
  splus_b2: 0.999
  splus_ema_rate: 0.999
  splus_inverse_every: 100
  splus_max_dim: 10000

# Learning rate scheduler
scheduler:
  name: "cosine"  # Options: "cosine", "linear", "constant"
  warmup_steps: 1000
  min_lr_ratio: 0.1  # Final LR = learning_rate * min_lr_ratio

# Training loop configuration
# Training is step-based with "pseudo-epochs" for validation/checkpointing
training:
  max_steps: 300000  # Total training steps
  steps_per_epoch: 3000  # Steps per pseudo-epoch (validation interval)
  batch_size: 2048
  gradient_clip: 1.0
  gradient_accumulation_steps: 1
  max_eval_steps: 200  # Max validation steps (prevents infinite loop)
  # JAX-specific options
  jit_compile: true  # JIT compile train/eval steps
  use_pmap: false  # Use pmap for multi-device (experimental)
  donate_argnums: true  # Donate buffers for memory efficiency
  # Mixed precision settings
  mixed_precision: true  # Enable bfloat16 mixed precision
  precision_dtype: "bfloat16"  # "bfloat16" or "float16"
  matmul_precision: "high"  # "default", "high" (TF32), or "highest" (full precision)

# Data loading
data:
  train_path: "~/training_bag/training-run1-test80-202507*.bag"
  val_path: "~/training_bag/training-run1-test80-202508*.bag"
  num_workers: 4
  prefetch_factor: 10
  seed: 42

# Checkpointing
checkpoint:
  dir: "checkpoints_jax"
  save_every_epochs: 1  # Save every N pseudo-epochs
  keep_last: 5
  save_optimizer: true
  use_orbax: true  # Use Orbax for checkpointing (recommended)

# Weights & Biases logging
wandb:
  enabled: true
  project: "catgpt-leela"
  entity: null  # Your W&B username or team
  run_name: null  # Auto-generated if null
  tags: ["jax"]
  log_every_steps: 10

# Distributed training (JAX-specific)
distributed:
  enabled: false
  mesh_shape: null  # e.g., [2, 4] for 2x4 device mesh
  data_axis: "data"  # Name of data parallelism axis

# Logging
logging:
  level: "INFO"

# Reproducibility
seed: 42
